{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RaSPLab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <b><font color='#009e74'>Rapid protein stability prediction using deep learning representations </font></b>\n",
        "\n",
        "Preprint pipeline version for predicting protein variants **thermodynamic stability changes** ($\\Delta \\Delta G$) using a deep learning representation. The program, using as input a protein structure (uploaded as PDB) returns stability predictions ($\\Delta \\Delta G$ in kcal/mol) for each variant at each position of the query protein.\n",
        "More details can be found in: **Blaabjerg et al.:** [\"Rapid protein stability prediction using deep learning representations\"](https://www.biorxiv.org/content/10.1101/2022.07.14.500157v1). Source code is available on the project [Github](https://github.com/KULL-Centre/papers/tree/main/2022/ML-ddG-Blaabjerg-et-al) page.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8szWTVF-dXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <b><font color='#009e74'> Reminders and Important informations:</font></b>\n",
        "- This notebook  <b><font color='#d55c00'>must</font></b> be run in a Colab GPU session (go to page menu: `Runtime`->  `Change runtime type` -> select `GPU` and confirm\n",
        "- Cells named as  <b><font color='#56b4e9'>PRELIMINARY OPERATIONS </font></b> have to be run <b><font color='#d55c00'>ONCE only at the start</font></b>  and  skipped for new predictions.\n",
        "- <b><font color='#d55c00'>ONE</font></b> single pdb at the time can be processed by the pipeline. \n",
        "- A  <b><font color='#d55c00'>new run</font></b> can be perform input direcly the new structure in the pdb upload cell and run the prediction cell again\n",
        "\n",
        "****"
      ],
      "metadata": {
        "id": "z2NZN9xv_2Gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b><font color='#009e74'>PIPELINE : PRELIMINARY OPERATIONS </font></b>\n",
        "These cells should be run once at the start of the notebook\n",
        "****"
      ],
      "metadata": {
        "id": "I0tOqP4RNHbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color='#56b4e9'>PRELIMINARY OPERATIONS</font>: Setup enviroment and dependencies</b>\n",
        "\n",
        "#@markdown Run this cell to install the required enviroment and dependencies\n",
        "\n",
        "#@markdown **N.B: This cell should be run only ONCE at the START of the notebook.**\n",
        "%%bash\n",
        "rm -r sample_data\n",
        "\n",
        "#install minconda\n",
        "env PYTHONPATH= &> /dev/null\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-py37_4.9.2-Linux-x86_64.sh &> /dev/null\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT &> /dev/null\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT &> /dev/null\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX  &> /dev/null\n",
        "\n",
        "rm /content/Miniconda3-py37_4.9.2-Linux-x86_64.sh\n",
        "\n",
        "#update miniconda to latest 3.7\n",
        "conda install --channel defaults conda python=3.7 --yes &> /dev/null\n",
        "conda update --channel defaults --all --yes &> /dev/null\n",
        "\n",
        "echo \"-> Anaconda installed!\"\n",
        "\n",
        "#install dependencies\n",
        "\n",
        "# install dependencies present in pip\n",
        "pip install numpy==1.17.3 torch==1.2.0 biopython==1.72 matplotlib==3.1.1 pdb-tools &> /dev/null\n",
        "pip install --upgrade pdb-tools &> /dev/null\n",
        "\n",
        "# install remaining dependencies with conda\n",
        "conda install mpl-scatter-density pdbfixer=1.5 openmm=7.3.1 ptitprince -c omnia -c conda-forge -c anaconda -c defaults --yes &> /dev/null\n",
        "\n",
        "echo \"--> Extra python packages installed!\"\n",
        "\n",
        "#install svn\n",
        "apt-get install -qq subversion &> /dev/null\n",
        "\n",
        "#mkdir of necessary folders\n",
        "mkdir data\n",
        "mkdir data/test\n",
        "mkdir data/test/predictions\n",
        "mkdir data/test/predictions/raw\n",
        "mkdir data/test/predictions/cleaned\n",
        "mkdir data/test/predictions/parsed\n",
        "mkdir output/\n",
        "mkdir output/predictions\n",
        "\n",
        "#download project folders from github\n",
        "\n",
        "svn checkout https://github.com/KULL-Centre/papers/trunk/2022/ML-ddG-Blaabjerg-et-al/src  &> /dev/null\n",
        "svn checkout https://github.com/KULL-Centre/papers/trunk/2022/ML-ddG-Blaabjerg-et-al/output/cavity_models  &> /dev/null\n",
        "svn checkout https://github.com/KULL-Centre/papers/trunk/2022/ML-ddG-Blaabjerg-et-al/output/ds_models  &> /dev/null\n",
        "\n",
        "wget -cq https://github.com/KULL-Centre/papers/raw/papers/2022/ML-ddG-Blaabjerg-et-al/data/pdb_frequencies.npz -o /content/data/pdb_frequencies.npz\n",
        "wget -cq https://github.com/KULL-Centre/papers/raw/main/2022/ML-ddG-Blaabjerg-et-al/colab_additonals/colab_additional.zip\n",
        "\n",
        "#extra files for runnin the notebooks\n",
        "\n",
        "mv ds_models ./output/\n",
        "mv cavity_models ./output/\n",
        "\n",
        "unzip colab_additional.zip &> /dev/null\n",
        "rm colab_additional.zip\n",
        "\n",
        "mv /content/colab_additional/best_model_path.txt /content/output/cavity_models/\n",
        "mv /content/colab_additional/clean_pdb.py /content/src/pdb_parser_scripts/\n",
        "mv /content/colab_additional/helpers.py /content/src/\n",
        "mv /content/colab_additional/pdb_frequencies.npz /content/data/\n",
        "\n",
        "echo \"---> Github data imported!\"\n",
        "\n",
        "#get and compile reduce\n",
        "\n",
        "cd src/pdb_parser_scripts\n",
        "git clone https://github.com/rlabduke/reduce.git\n",
        "cd reduce/\n",
        "make &> /dev/null\n",
        "\n",
        "mv /content/colab_additional/reduce /content/src/pdb_parser_scripts/reduce/\n",
        "\n",
        "chmod +x /content/src/pdb_parser_scripts/reduce/reduce \n",
        "echo \"----> reduce installed\"\n",
        "\n",
        "rm -r /content/colab_additional\n",
        "#@markdown ****"
      ],
      "metadata": {
        "id": "NTRMe6SmnAW0",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color='#56b4e9'>PRELIMINARY OPERATIONS</font>: Import python libraries, functions and setup common variables</b>\n",
        "\n",
        "#@markdown Run this cell to import libraries and functions necessary for the pipeline.\n",
        "\n",
        "#@markdown **N.B: This cell should be run only ONCE at the START of the notebook.**\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/usr/local/lib/python3.7/site-packages\")\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import pathlib\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "import datetime\n",
        "import matplotlib\n",
        "from pdbfixer import PDBFixer\n",
        "from simtk.openmm.app import PDBFile\n",
        "from Bio.PDB.Polypeptide import index_to_one, one_to_index\n",
        "from collections import OrderedDict\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from google.colab import files\n",
        "\n",
        "sys.path.append('./src/')\n",
        "\n",
        "from cavity_model import (\n",
        "     CavityModel,\n",
        "     DownstreamModel,\n",
        "     ResidueEnvironment,\n",
        "     ResidueEnvironmentsDataset,\n",
        ")\n",
        "\n",
        "from helpers import (\n",
        "     populate_dfs_with_resenvs,\n",
        "     remove_disulfides,\n",
        "     fermi_transform,\n",
        "     inverse_fermi_transform,\n",
        "     init_lin_weights,\n",
        "     ds_pred,\n",
        "     cavity_to_prism,\n",
        "     get_seq_from_variant,\n",
        ")\n",
        "\n",
        "from visualization import (\n",
        "     hist_plot,\n",
        ")\n",
        "\n",
        "#Extra function to fix pdb\n",
        "\n",
        "# Setup pipeline parameters\n",
        "## Set seeds\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "## Main deep parameters\n",
        "DEVICE = \"cuda\"  # \"cpu\" or \"cuda\"\n",
        "NUM_ENSEMBLE = 10\n",
        "TASK_ID = int(1)\n",
        "PER_TASK = int(1)\n",
        "\n",
        "#@markdown ****"
      ],
      "metadata": {
        "id": "QKz_yq3VxDCR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b><font color='#009e74'>PIPELINE : PREDICTIONS </font></b>"
      ],
      "metadata": {
        "id": "OpUMk4UezkLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color='#56b4e9'> PDB upload</font></b>\n",
        "\n",
        "#@markdown Choose between <b><font color='#d55c00'> ONE</font></b> of the possible input sources for the target pdb and <ins>leave the other cells empty or unmarked</ins>\n",
        "#@markdown - AlphaFold2 PDB via Uniprot ID:\n",
        "AF_ID ='P04036'#@param {type:\"string\"}\n",
        "#@markdown - PDB ID (imported from RCSB PDB):\n",
        "PDB_ID =''#@param {type:\"string\"}\n",
        "#@markdown - Upload custom PDB\n",
        "PDB_custom =False#@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown Select target chain (default A)\n",
        "chain='A' #@param {type:'string'}\n",
        "\n",
        "if os.path.exists(\"/content/query_protein.pdb\"):\n",
        "    os.remove(\"/content/query_protein.pdb\")\n",
        "if os.path.exists(\"/content/data/test/predictions/raw/query_protein_uniquechain.pdb\"):\n",
        "    os.remove(\"/content/data/test/predictions/raw/query_protein_uniquechain.pdb\")\n",
        "if os.path.exists(\"/content/data/test/predictions/cleaned/query_protein_uniquechain_clean.pdb\"):\n",
        "    os.remove(\"/content/data/test/predictions/cleaned/query_protein_uniquechain_clean.pdb\")\n",
        "if os.path.exists(\"/content/data/test/predictions/parsed/query_protein_uniquechain_clean_coordinate_features.npz\"):\n",
        "    os.remove(\"/content/data/test/predictions/parsed/query_protein_uniquechain_clean_coordinate_features.npz\")\n",
        "\n",
        "if (AF_ID !='') and (len(AF_ID)==6) : \n",
        "    subprocess.call(['curl','-s','-f',f'https://alphafold.ebi.ac.uk/files/AF-{AF_ID}-F1-model_v2.pdb','-o','/content/query_protein.pdb'])\n",
        "elif (PDB_ID !='') and (len(PDB_ID)==4):\n",
        "    subprocess.call(['curl','-s','-f',f'https://files.rcsb.org/download/{PDB_ID}.pdb','-o','/content/query_protein.pdb'])\n",
        "\n",
        "elif PDB_custom:\n",
        "  print('Upload PDB file:')\n",
        "  uploaded_pdb = files.upload()\n",
        "  for fn in uploaded_pdb.keys():\n",
        "    os.rename(fn, f\"/content/query_protein.pdb\")\n",
        "    print('PDB file correctly loaded')\n",
        "\n",
        "else:\n",
        "  print(f'ERROR: any PDB uploaded, please select one of the above inputs')\n",
        "\n",
        "\n",
        "\n",
        "#@markdown N.B. This cell will also perform preliminary operations to correcly format the uploaded PDB\n",
        "\n",
        "## remove other chains and move to raw folder\n",
        "!pdb_selchain -\"$chain\" /content/query_protein.pdb | pdb_delhetatm | pdb_delres --999:0:1 | pdb_fixinsert | pdb_tidy  > /content/data/test/predictions/raw/query_protein_uniquechain.pdb\n",
        "# Select PDBs to run during this task - could be simplified if we decide to set PER_TASK = 1 for all cases\n",
        "\n",
        "pdb_input_dir = \"data/test/predictions/raw/\"\n",
        "input_pdbs = sorted(list(filter(lambda x: x.endswith(\".pdb\"), os.listdir('data/test/predictions/raw/'))))\n",
        "start = (TASK_ID-1)*(PER_TASK)\n",
        "end = (TASK_ID*PER_TASK)\n",
        "if end > len(input_pdbs):\n",
        "    end = len(input_pdbs) #avoid end index exceeding length of list\n",
        "pdbs = input_pdbs[start:end] \n",
        "pdb_names = [i.split(\".\")[0] for i in pdbs]\n",
        "print(pdb_names)\n",
        "print(f\"Pre-processing PDBs ...\")\n",
        "\n",
        "!python3 /content/src/pdb_parser_scripts/clean_pdb.py --pdb_file_in /content/data/test/predictions/raw/query_protein_uniquechain.pdb --out_dir /content/data/test/predictions/cleaned/ --reduce_exe /content/src/pdb_parser_scripts/reduce/reduce #&> /dev/null\n",
        "!python3 /content/src/pdb_parser_scripts/extract_environments.py --pdb_in /content/data/test/predictions/cleaned/query_protein_uniquechain_clean.pdb  --out_dir /content/data/test/predictions/parsed/  #&> /dev/null\n",
        "\n",
        "if os.path.exists(\"/content/data/test/predictions/parsed/query_protein_uniquechain_clean_coordinate_features.npz\"):\n",
        "  print(f\"Pre-processing PDBs correctly ended\")\n",
        "else:\n",
        "  print(f\"Pre-processing PDB didn't end correcly, please check input informations\")\n",
        "\n",
        "#@markdown ****"
      ],
      "metadata": {
        "id": "Z8nUmHI5rgjy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "84bb49eb-6791-4c9f-98be-1e59f64fad6a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['query_protein_uniquechain']\n",
            "Pre-processing PDBs ...\n",
            "Time for cleaning /content/data/test/predictions/raw/query_protein_uniquechain.pdb: 3.977048635482788\n",
            "/usr/local/lib/python3.7/site-packages/Bio/PDB/Vector.py:42: BiopythonDeprecationWarning: The module Bio.PDB.Vector has been deprecated in favor of new module Bio.PDB.vectors to solve a name collision with the class Vector. For the class Vector, and vector functions like calc_angle, import from Bio.PDB instead.\n",
            "  \"import from Bio.PDB instead.\", BiopythonDeprecationWarning)\n",
            "/content/src/pdb_parser_scripts/extract_environments.py:107: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  features[\"res_indices\"] = np.array(features[\"res_indices\"], dtype=np.int)\n",
            "Time for parsing environments from /content/data/test/predictions/cleaned/query_protein_uniquechain_clean.pdb: 0.3372001647949219\n",
            "Pre-processing PDBs correctly ended\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title <b><font color='#56b4e9'> Pipeline RUN </font></b>\n",
        "\n",
        "#@markdown <b><font color='#d55c00'>Execute the cell</font></b> to run the pipeline and generate **saturation mutagenesis predictions of thermodynamic stability changes** predictions\n",
        "\n",
        "### Pre-process structure data\n",
        "\n",
        "# Create temporary residue environment datasets to more easily match ddG data\n",
        "pdb_filenames_ds = sorted(glob.glob(f\"/content/data/test/predictions/parsed/*coord*\"))\n",
        "\n",
        "dataset_structure = ResidueEnvironmentsDataset(pdb_filenames_ds, transformer=None)\n",
        "\n",
        "resenv_dataset = {}\n",
        "for resenv in dataset_structure:\n",
        "    if AF_ID!='':\n",
        "      key = (f\"--{AF_ID}--{resenv.chain_id}--{resenv.pdb_residue_number}--{index_to_one(resenv.restype_index)}--\"\n",
        "          )\n",
        "    elif PDB_ID!='':\n",
        "      key = (f\"--{PDB_ID}--{resenv.chain_id}--{resenv.pdb_residue_number}--{index_to_one(resenv.restype_index)}--\"\n",
        "          )\n",
        "    else:\n",
        "      key = (f\"--{'CUSTOM'}--{resenv.chain_id}--{resenv.pdb_residue_number}--{index_to_one(resenv.restype_index)}--\"\n",
        "          )\n",
        "    resenv_dataset[key] = resenv\n",
        "df_structure_no_mt = pd.DataFrame.from_dict(resenv_dataset, orient='index', columns=[\"resenv\"])\n",
        "df_structure_no_mt.reset_index(inplace=True)\n",
        "df_structure_no_mt[\"index\"]=df_structure_no_mt[\"index\"].astype(str)\n",
        "res_info = pd.DataFrame(df_structure_no_mt[\"index\"].str.split('--').tolist(),\n",
        "                        columns = ['blank','pdb_id','chain_id','pos','wt_AA', 'blank2'])\n",
        "\n",
        "df_structure_no_mt[\"pdbid\"] = res_info['pdb_id']\n",
        "df_structure_no_mt[\"chainid\"] = res_info['chain_id']\n",
        "df_structure_no_mt[\"variant\"] = res_info[\"wt_AA\"] + res_info['pos'] + \"X\"\n",
        "aa_list = [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \n",
        "            \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]\n",
        "df_structure = pd.DataFrame(df_structure_no_mt.values.repeat(20, axis=0), columns=df_structure_no_mt.columns)\n",
        "for i in range(0, len(df_structure), 20):\n",
        "    for j in range(20):\n",
        "        df_structure.iloc[i+j, :][\"variant\"] = df_structure.iloc[i+j, :][\"variant\"][:-1] + aa_list[j]\n",
        "df_structure.drop(columns=\"index\", inplace=True)\n",
        "\n",
        "# Load PDB amino acid frequencies used to approximate unfolded states\n",
        "pdb_nlfs = -np.log(np.load(f\"{os.getcwd()}/data/pdb_frequencies.npz\")[\"frequencies\"])\n",
        "\n",
        "# # Add wt and mt idxs and freqs to df\n",
        "\n",
        "df_structure[\"wt_idx\"] = df_structure.apply(lambda row: one_to_index(row[\"variant\"][0]), axis=1)\n",
        "df_structure[\"mt_idx\"] = df_structure.apply(lambda row: one_to_index(row[\"variant\"][-1]), axis=1)\n",
        "df_structure[\"wt_nlf\"] = df_structure.apply(lambda row: pdb_nlfs[row[\"wt_idx\"]], axis=1)\n",
        "df_structure[\"mt_nlf\"] = df_structure.apply(lambda row: pdb_nlfs[row[\"mt_idx\"]], axis=1)\n",
        "\n",
        "# Define models\n",
        "best_cavity_model_path = open(f\"/content/output/cavity_models/best_model_path.txt\", \"r\").read()\n",
        "cavity_model_net = CavityModel(DEVICE, get_latent=True).to(DEVICE)\n",
        "cavity_model_net.load_state_dict(torch.load(f\"{best_cavity_model_path}\"))\n",
        "cavity_model_net.eval()\n",
        "ds_model_net = DownstreamModel().to(DEVICE)\n",
        "ds_model_net.apply(init_lin_weights)\n",
        "ds_model_net.eval()\n",
        "\n",
        "###set start time\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "# Make ML predictions\n",
        "print(f\"Starting downstream model prediction\")\n",
        "dataset_key=\"predictions\"\n",
        "df_ml = ds_pred(cavity_model_net,\n",
        "                ds_model_net,\n",
        "                NUM_ENSEMBLE,\n",
        "                DEVICE,\n",
        "                df_structure,\n",
        "                dataset_key,\n",
        "                ) \n",
        "print(f\"Finished downstream model prediction\")\n",
        "end_time = time.perf_counter()\n",
        "elapsed = datetime.timedelta(seconds = end_time - start_time)\n",
        "print(\"Complete - prediction execution took\", elapsed)\n",
        "\n",
        "elapsed = datetime.timedelta(seconds = end_time - start_time)\n",
        "print(\"Generating output files\")\n",
        "#Merge and save data with predictions\n",
        "\n",
        "df_total = df_structure.merge(df_ml, on=['pdbid','chainid','variant'], how='outer')\n",
        "#df_total[\"b_factors\"] = df_total.apply(lambda row: row[\"resenv\"].b_factors, axis=1)\n",
        "df_total = df_total.drop(\"resenv\", 1)\n",
        "print(f\"{len(df_structure)-len(df_ml)} data points dropped when matching total data with ml predictions in: {dataset_key}.\")\n",
        "\n",
        "# Parse output into separate files by pdb, print to PRISM format\n",
        "for pdbid in df_total[\"pdbid\"].unique():\n",
        "    df_pdb = df_total[df_total[\"pdbid\"]==pdbid]\n",
        "    for chainid in df_pdb[\"chainid\"].unique():\n",
        "        pred_outfile = f\"{os.getcwd()}/output/{dataset_key}/cavity_pred_{pdbid}_{chainid}.csv\"\n",
        "        print(f\"Parsing predictions from pdb: {pdbid}{chainid} into {pred_outfile}\")\n",
        "        df_chain = df_pdb[df_pdb[\"chainid\"]==chainid]\n",
        "        df_chain = df_chain.assign(pos = df_chain[\"variant\"].str[1:-1])\n",
        "        df_chain['pos'] = pd.to_numeric(df_chain['pos'])\n",
        "        first_res_no = min(df_chain[\"pos\"])\n",
        "        df_chain = df_chain.assign(wt_AA = df_chain[\"variant\"].str[0])\n",
        "        df_chain = df_chain.assign(mt_AA = df_chain[\"variant\"].str[-1])\n",
        "        seq = get_seq_from_variant(df_chain)\n",
        "        df_chain.to_csv(pred_outfile, index=False)\n",
        "        prism_outfile = f\"/content/output/{dataset_key}/prism_cavity_{pdbid}_{chainid}.txt\"\n",
        "\n",
        "        # if (AF_ID !=''):\n",
        "        #   prism_outfile = f\"/content/output/{dataset_key}/prism_cavity_{AF_ID}_{chainid}.txt\"\n",
        "        # elif (PDB_ID !=''):\n",
        "        #   prism_outfile = f\"/content/output/{dataset_key}/prism_cavity_{PDB_ID}_{chainid}.txt\"\n",
        "        # elif PDB_custom:\n",
        "        #   prism_outfile = f\"/content/output/{dataset_key}/prism_cavity_XXXX_{chainid}.txt\"\n",
        "        cavity_to_prism(df_chain, pdbid, chainid, seq, prism_outfile)\n",
        "\n",
        "# End timer and print result\n",
        "#!rm /content/output/predictions/*xxxx*.csv\n",
        "elapsed = datetime.timedelta(seconds = end_time - start_time)\n",
        "print(\"Complete - files generated\")\n",
        "\n",
        "#@markdown ****"
      ],
      "metadata": {
        "id": "tAxW8XNqxCS_",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><font color='#56b4e9'> Download results as archive </font></b>\n",
        "\n",
        "#@markdown Run the cell to <b><font color='#009e74'> download a .zip archive </font></b> with prediction files for the <ins>current run</ins>.\n",
        "\n",
        "#@markdown <ins>Tick</ins> the next box if you ran multiple predictions and you want to <ins>download all of them</ins>.\n",
        "\n",
        "download_all_predictions= False #@param {type:\"boolean\"}\n",
        "\n",
        "if download_all_predictions:\n",
        "  os.system( \"zip -r {} {}\".format( f\"predictions_output_all.zip\" , f\"/content/output/predictions/*\" ) )\n",
        "  files.download(f\"predictions_output_all.zip\")\n",
        "else:\n",
        "  if (AF_ID !=''):\n",
        "    os.system( \"zip -r {} {}\".format( f\"predictions_output_{AF_ID}.zip\" , f\"/content/output/predictions/*{AF_ID}*\" ) )\n",
        "    files.download(f\"predictions_output_{AF_ID}.zip\")\n",
        "  elif (PDB_ID !=''):\n",
        "    os.system( \"zip -r {} {}\".format( f\"predictions_output_{PDB_ID}.zip\" , f\"/content/output/predictions/*{PDB_ID}*\" ) )\n",
        "    files.download(f\"predictions_output_{PDB_ID}.zip\")\n",
        "  else:\n",
        "    os.system( \"zip -r {} {}\".format( f\"predictions_output.zip\" , f\"/content/output/predictions\" ) )\n",
        "    files.download(f\"predictions_output.zip\")\n",
        "\n",
        "  if download_all_predictions:\n",
        "    os.system( \"zip -r {} {}\".format( f\"predictions_output.zip\" , f\"/content/output/predictions\" ) )\n",
        "    files.download(f\"predictions_output_all.zip\")\n",
        "\n",
        "#@markdown **P.S.: prediction files are also stored in the colab file system folder: `/output/predictions/`**\n",
        "#@markdown ****"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "tVAoMFcNlasI",
        "outputId": "145b112c-fcbf-4ceb-e570-fe81ab2e03ea",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_12ad68e0-fe08-4de1-bf99-dce541a0fe2f\", \"predictions_output_7OGJ.zip\", 194273)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Troubleshooting**\n",
        "\n",
        "- Check that the runtime type is set to GPU at \"Runtime\" -> \"Change runtime type\".\n",
        "- Try to restart the session \"Runtime\" -> \"Factory reset runtime\".\n",
        "- Check your input pdb.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Known problems:**\n",
        "\n",
        "- Residues with numeration index below 0 are not supported by the output file parser and thus they deleted from the pdb in the pre-processing step.\n",
        "- Insertion annotations in the pdb are not supported. Any annotations is actually deleted during the pre-processing step.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**License:**\n",
        "\n",
        "RaSP's source code is licensed under the permissive Apache Licence, Version 2.0.\n",
        " Additionally, this notebook uses the reduce source code which license could be find in `/content/src/pdb_parser_scripts/reduce/`\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Bugs:**\n",
        "\n",
        "For any bugs please report the issue on the project [Github](https://github.com/KULL-Centre/papers/tree/main/2022/ML-ddG-Blaabjerg-et-al) or contact one of the listed authors in the connected [manuscript](https://www.biorxiv.org/content/10.1101/2022.07.14.500157v1).\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Citing this work:**\n",
        "\n",
        "If you use our model please cite:\n",
        "\n",
        "Blaabjerg, L.M., Kassem, M.M., Good, L.L., Jonsson, N., Cagiada, M., Johansson, K.E., Boomsma, W., Stein, A. and Lindorff-Larsen, K., 2022. *Rapid protein stability prediction using deep learning representations*, bioRxiv. (https://doi.org/10.1101/2022.07.14.500157)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "@article{blaabjerg2022rapid,\n",
        "  title={Rapid protein stability prediction using deep learning representations},\n",
        "  author={Blaabjerg, Lasse M and Kassem, Maher M and Good, Lydia L and Jonsson, Nicolas and Cagiada, Matteo and Johansson, Kristoffer E and Boomsma, Wouter and Stein, Amelie and Lindorff-Larsen, Kresten},\n",
        "  journal={bioRxiv},\n",
        "  year={2022},\n",
        "  publisher={Cold Spring Harbor Laboratory}\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1fb2HFDWC2Yu"
      }
    }
  ]
}